{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kmnp0JT1Ngty"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTPQZ-mLNgtz",
        "outputId": "2a896c3b-14f6-47e4-d320-73c3ecd69cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# check CUDA\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CgsbrbZPNgtz"
      },
      "outputs": [],
      "source": [
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1) # (28, 28)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)   # (28, 28)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)  # (28, 28)->(14, 14)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)  # (14, 14)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, padding=1)  # （14， 14） -> (7, 7)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv6 = nn.Conv2d(512, 256, kernel_size=3, padding=1)  # (7, 7)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=3, padding=1)  #  (7, 7)\n",
        "        self.bn7 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv8 = nn.Conv2d(128, 32, kernel_size=3, padding=1)  #  (7, 7)\n",
        "        self.bn8 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 64)\n",
        "        self.fc2 = nn.Linear(64, 15)  # 15 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (F.relu(self.bn1(self.conv1(x))))   #1\n",
        "        x = (F.relu(self.bn2(self.conv2(x))))   #2\n",
        "        #x = self.dropout(x)\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  #3\n",
        "        x = (F.relu(self.bn4(self.conv4(x))))   #4\n",
        "        x = self.pool(F.relu(self.bn5(self.conv5(x))))  #5\n",
        "        x = self.dropout(x)\n",
        "        x = (F.relu(self.bn6(self.conv6(x))))   #6\n",
        "        x = (F.relu(self.bn7(self.conv7(x))))   #7\n",
        "        x = (F.relu(self.bn8(self.conv8(x))))   #8\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# First, unzip the file\n",
        "#with zipfile.ZipFile('../data.npz.zip', 'r') as zip_ref:\n",
        "#    zip_ref.extractall()  # You can specify a directory path here if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# load the dataset \n",
        "\n",
        "data = np.load('data.npz')\n",
        "X_train = data['X_train'].astype(np.float32)  # Shape: (82875, 784)\n",
        "y_train = data['y_train'].astype(np.int64)    # Shape: (82875,)\n",
        "X_test = data['X_test'].astype(np.float32)    # Shape: (14625, 784)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D_yshV9qNgtz"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "X_scaled = X_train/255\n",
        "X_scaled = X_scaled.reshape(-1, 1, 28, 28)  # Reshape back to (n_samples, channels, height, width)\n",
        "\n",
        "X_test_scaled = X_test/255\n",
        "X_test_scaled = X_test_scaled.reshape(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "noDvxBoQNgt0"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pkyeA6-ENgt0"
      },
      "outputs": [],
      "source": [
        "# Convert arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "X_val_tensor = torch.tensor(X_val)\n",
        "y_val_tensor = torch.tensor(y_val)\n",
        "X_test_tensor = torch.tensor(X_test_scaled)\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KPZ21MdpNgt0"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9, eps=1e-09,weight_decay=0,momentum=0.9,centered=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WgwNhDn3Ngt0"
      },
      "outputs": [],
      "source": [
        "# Training model\n",
        "def train(num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for data, targets in train_loader:\n",
        "            data, targets = data.to(device), targets.to(device) # Move data to GPU if available\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "# Validate the model\n",
        "def validate_model():\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)  # Move data to GPU if available\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Prediction function\n",
        "def predict(data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            outputs = model(data)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    return all_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR3WTWpjNgt0",
        "outputId": "1613b1f0-890a-47e3-97f0-522574092403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.14749841392040253\n",
            "Epoch 2, Loss: 0.03512042015790939\n",
            "Epoch 3, Loss: 0.12077698856592178\n",
            "Epoch 4, Loss: 0.0295538492500782\n",
            "Epoch 5, Loss: 0.04331273213028908\n",
            "Epoch 6, Loss: 0.040117621421813965\n",
            "Epoch 7, Loss: 0.03122521936893463\n",
            "Epoch 8, Loss: 0.08467192202806473\n",
            "Epoch 9, Loss: 0.0029725513886660337\n",
            "Epoch 10, Loss: 0.12311655282974243\n",
            "Epoch 11, Loss: 0.004145391751080751\n",
            "Epoch 12, Loss: 0.009853522293269634\n",
            "Epoch 13, Loss: 0.050638582557439804\n",
            "Epoch 14, Loss: 0.011532285250723362\n",
            "Epoch 15, Loss: 0.011622577905654907\n",
            "Epoch 16, Loss: 0.002295181853696704\n",
            "Epoch 17, Loss: 0.012755406089127064\n",
            "Epoch 18, Loss: 0.00031205645063892007\n",
            "Epoch 19, Loss: 0.0003736028738785535\n",
            "Epoch 20, Loss: 0.14515523612499237\n",
            "Validation Accuracy: 96.94%\n"
          ]
        }
      ],
      "source": [
        "# Run the training and validation\n",
        "train(20)\n",
        "validate_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uco7AXv0Ngt0",
        "outputId": "c950a981-34c1-499d-e797-910effd0ea9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14625,)\n"
          ]
        }
      ],
      "source": [
        "# Predict on the test set\n",
        "test_loader = DataLoader(X_test_tensor, batch_size=64, shuffle=False)\n",
        "test_predictions = predict(test_loader)\n",
        "\n",
        "print(np.shape(test_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "m_ppAmlPNgt0"
      },
      "outputs": [],
      "source": [
        "# # Save predictions\n",
        "# predict_id = np.arange(0, len(X_test))\n",
        "# submission_predictions = np.vstack((predict_id, test_predictions)).T\n",
        "\n",
        "# # Save the predictions to a CSV file\n",
        "# np.savetxt(\"cnn_predictions.csv\",submission_predictions, delimiter=\",\", fmt='%d', header=\"ID,Label\", comments='')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "3UBcHhhNVwwG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved as submission/cnn_predictions_20240414_231356.csv\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Assume test_predictions are available from your model's output\n",
        "\n",
        "# Create the submission directory if it doesn't exist\n",
        "directory = \"submission\"\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Get current date and time\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"cnn_predictions_{current_time}.csv\"\n",
        "\n",
        "# Full path for saving the file\n",
        "full_path = os.path.join(directory, filename)\n",
        "\n",
        "# Preparing the data to save\n",
        "predict_id = np.arange(0, len(X_test))\n",
        "submission_predictions = np.vstack((predict_id, test_predictions)).T\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "np.savetxt(full_path, submission_predictions, delimiter=\",\", fmt='%d', header=\"ID,Label\", comments='')\n",
        "\n",
        "print(f\"File saved as {full_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hUPGTkq3SzCD",
        "outputId": "aa37cc06-4378-47f0-f12d-ccfb8599ab15"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# files.download('cnn_predictions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SrUSy7YSTjE8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 28, 28]             640\n",
            "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
            "            Conv2d-3          [-1, 128, 28, 28]          73,856\n",
            "       BatchNorm2d-4          [-1, 128, 28, 28]             256\n",
            "            Conv2d-5          [-1, 256, 28, 28]         295,168\n",
            "       BatchNorm2d-6          [-1, 256, 28, 28]             512\n",
            "         MaxPool2d-7          [-1, 256, 14, 14]               0\n",
            "            Conv2d-8          [-1, 512, 14, 14]       1,180,160\n",
            "       BatchNorm2d-9          [-1, 512, 14, 14]           1,024\n",
            "           Conv2d-10          [-1, 512, 14, 14]       2,359,808\n",
            "      BatchNorm2d-11          [-1, 512, 14, 14]           1,024\n",
            "        MaxPool2d-12            [-1, 512, 7, 7]               0\n",
            "          Dropout-13            [-1, 512, 7, 7]               0\n",
            "           Conv2d-14            [-1, 256, 7, 7]       1,179,904\n",
            "      BatchNorm2d-15            [-1, 256, 7, 7]             512\n",
            "           Conv2d-16            [-1, 128, 7, 7]         295,040\n",
            "      BatchNorm2d-17            [-1, 128, 7, 7]             256\n",
            "           Conv2d-18             [-1, 32, 7, 7]          36,896\n",
            "      BatchNorm2d-19             [-1, 32, 7, 7]              64\n",
            "           Linear-20                   [-1, 64]         100,416\n",
            "          Dropout-21                   [-1, 64]               0\n",
            "           Linear-22                   [-1, 15]             975\n",
            "================================================================\n",
            "Total params: 5,526,639\n",
            "Trainable params: 5,526,639\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 9.50\n",
            "Params size (MB): 21.08\n",
            "Estimated Total Size (MB): 30.59\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "input_shape = (1, 28, 28)\n",
        "summary(model, input_shape)\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
